{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "#import ltc_model as ltc\n",
    "#from ctrnn_model import CTRNN, NODE, CTGRU\n",
    "import argparse\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Control Sliders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = r'G:\\code\\Y2S1\\CS3244\\Data\\col\\collated'\n",
    "counter = 0\n",
    "lowest_nest = 258"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSE DATA HANDLING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Pickle Extract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\code\\Y2S1\\CS3244\\Data\\col\\collated\n",
      "(120, 67, 258)\n",
      "[ 1.  1.  2.  2.  3.  3.  3.  4.  4.  4.  5.  5.  6.  6.  6.  7.  7.  7.\n",
      "  8.  8.  9.  9. 10. 10. 10. 11. 11. 12. 12. 12. 13. 13. 14. 14. 14. 14.\n",
      " 15. 15. 15. 16. 16. 17. 17. 17. 18. 18. 18. 19. 19. 19. 20. 20. 20. 20.\n",
      " 21. 21. 22. 22. 22. 23. 23. 24. 24. 25. 25. 25. 26. 26. 26. 26. 27. 27.\n",
      " 27. 28. 28. 29. 29. 29. 30. 30. 30. 31. 31. 31. 31. 31. 31. 31. 31. 31.\n",
      " 31. 31. 31. 32. 32. 32. 33. 33. 33. 34. 34. 34. 35. 35. 36. 36. 36. 36.\n",
      " 37. 37. 37. 38. 38. 38. 39. 39. 40. 40. 41. 41.]\n",
      "2\n",
      "Total number of training sequences: 405\n"
     ]
    }
   ],
   "source": [
    "def np_list_pad(list):\n",
    "    def find_shape(seq):\n",
    "        try:\n",
    "            len_ = len(seq)\n",
    "        except TypeError:\n",
    "            return ()\n",
    "        shapes = [find_shape(subseq) for subseq in seq]\n",
    "        return (len_,) + tuple(max(sizes) for sizes in zip_longest(*shapes,\n",
    "                                                                    fillvalue=1))\n",
    "\n",
    "    def fill_array(arr, seq):\n",
    "        if arr.ndim == 1:\n",
    "            try:\n",
    "                len_ = len(seq)\n",
    "            except TypeError:\n",
    "                len_ = 0\n",
    "            arr[:len_] = seq\n",
    "            arr[len_:] = np.nan\n",
    "        else:\n",
    "            for subarr, subseq in zip_longest(arr, seq, fillvalue=()):\n",
    "                fill_array(subarr, subseq)\n",
    "    arr = np.empty(find_shape(list))\n",
    "    fill_array(arr, list)\n",
    "    return arr\n",
    "\n",
    "class Data:\n",
    "    \n",
    "    def __init__(self,seq_len=16):\n",
    "        data_payload = {}\n",
    "        name_dict={}\n",
    "        counter = 0\n",
    "        global data_directory\n",
    "        print(data_directory)\n",
    "        train_x = []\n",
    "        train_y = []\n",
    "        test_x = []\n",
    "        test_y = []\n",
    "        for file in os.listdir(data_directory):\n",
    "            if file.endswith(\".pkl\"):\n",
    "                counter +=1 \n",
    "                file_directory = os.path.join(data_directory, file)\n",
    "                # print(os.path.join(data_directory, file))\n",
    "                object = pd.read_pickle(file_directory)\n",
    "                # print(object[1])\n",
    "                sep = '_'\n",
    "                stripped_file_name = file.split(sep, 1)[0]\n",
    "                name_dict[counter] = stripped_file_name\n",
    "                train, test = object[:int(len(object)*0.8)] , object[int(len(object)*0.8):]\n",
    "                train_x.extend(train)\n",
    "                test_x.extend(test)\n",
    "                train_y.extend([counter]*len(train))\n",
    "                test_y.extend([counter]*len(test))\n",
    "                data_payload[stripped_file_name] = object\n",
    "            #print(data_payload)\n",
    "            # Dictonary : {Phrase: 3d array}\n",
    "            # 3d array nesting is as follow: Scene,Frame, Flattened dataPoints\n",
    "\n",
    "        train_x = np_list_pad(train_x)\n",
    "        train_y = np_list_pad(train_y)\n",
    "        test_x = np_list_pad(test_x)\n",
    "        test_y = np_list_pad(test_y)\n",
    "        print(test_x.shape)\n",
    "        print(test_y)\n",
    "        print(len(train_x[:2]))\n",
    "\n",
    "        print(\"Total number of training sequences: {}\".format(train_x.shape[0]))\n",
    "        # permutation = np.random.RandomState(893429).permutation(train_x.shape[0])\n",
    "        # valid_size = int(0.1*train_x.shape[0])\n",
    "        # print(\"Validation split: {}, training split: {}\".format(valid_size,train_x.shape[0]-valid_size))\n",
    "        # print(permutation)\n",
    "        # self.valid_x = train_x[:,permutation[:valid_size]]\n",
    "        # self.valid_y = train_y[:,permutation[:valid_size]]\n",
    "        # self.train_x = train_x[:,permutation[valid_size:]]\n",
    "        # self.train_y = train_y[:,permutation[valid_size:]]\n",
    "\n",
    "        # self.test_x = test_x\n",
    "        # self.test_y = test_y\n",
    "        # print(\"Total number of test sequences: {}\".format(self.test_x.shape[1]))\n",
    "    \n",
    "    def iterate_train(self,batch_size=16):\n",
    "        total_seqs = self.train_x.shape[1]\n",
    "        permutation = np.random.permutation(total_seqs)\n",
    "        total_batches = total_seqs // batch_size\n",
    "\n",
    "        for i in range(total_batches):\n",
    "            start = i*batch_size\n",
    "            end = start + batch_size\n",
    "            batch_x = self.train_x[:,permutation[start:end]]\n",
    "            batch_y = self.train_y[:,permutation[start:end]]\n",
    "            yield (batch_x,batch_y)\n",
    "        \n",
    "data = Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Pickle Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Replace array_values line with this if you dont have pandas:\n",
    "\n",
    "infile = open(r'G:\\code\\Y2S1\\CS3244\\Data\\col\\collated\\afraid_scenes.pkl', 'rb')\n",
    "array_values = pickle.load(infile)\n",
    "infile.close()\n",
    "# print(len(array_values))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Model:\n",
    "\n",
    "    def __init__(self,model_type,model_size,learning_rate = 0.001):\n",
    "        self.model_type = model_type\n",
    "        self.constrain_op = None\n",
    "        self.x = tf.placeholder(dtype=tf.float32,shape=[None,None,lowest_nest])\n",
    "        self.target_y = tf.placeholder(dtype=tf.int32,shape=[None,None])\n",
    "\n",
    "        self.model_size = model_size\n",
    "        head = self.x\n",
    "        if(model_type == \"lstm\"):\n",
    "            self.fused_cell = tf.nn.rnn_cell.LSTMCell(model_size)\n",
    "            head,_ = tf.nn.dynamic_rnn(self.fused_cell,head,dtype=tf.float32,time_major=True)\n",
    "        elif(model_type.startswith(\"ltc\")):\n",
    "            learning_rate = 0.01 # LTC needs a higher learning rate\n",
    "            self.wm = ltc.LTCCell(model_size)\n",
    "            if(model_type.endswith(\"_rk\")):\n",
    "                self.wm._solver = ltc.ODESolver.RungeKutta\n",
    "            elif(model_type.endswith(\"_ex\")):\n",
    "                self.wm._solver = ltc.ODESolver.Explicit\n",
    "            else:\n",
    "                self.wm._solver = ltc.ODESolver.SemiImplicit\n",
    "\n",
    "            head,_ = tf.nn.dynamic_rnn(self.wm,head,dtype=tf.float32,time_major=True)\n",
    "            self.constrain_op = self.wm.get_param_constrain_op()\n",
    "        elif(model_type == \"node\"):\n",
    "            self.fused_cell = NODE(model_size,cell_clip=-1)\n",
    "            head,_ = tf.nn.dynamic_rnn(self.fused_cell,head,dtype=tf.float32,time_major=True)\n",
    "        elif(model_type == \"ctgru\"):\n",
    "            self.fused_cell = CTGRU(model_size,cell_clip=-1)\n",
    "            head,_ = tf.nn.dynamic_rnn(self.fused_cell,head,dtype=tf.float32,time_major=True)\n",
    "        elif(model_type == \"ctrnn\"):\n",
    "            self.fused_cell = CTRNN(model_size,cell_clip=-1,global_feedback=True)\n",
    "            head,_ = tf.nn.dynamic_rnn(self.fused_cell,head,dtype=tf.float32,time_major=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model type '{}'\".format(model_type))\n",
    "\n",
    "\n",
    "        self.y = tf.layers.Dense(6,activation=None)(head)\n",
    "        print(\"logit shape: \",str(self.y.shape))\n",
    "        self.loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(\n",
    "            labels = self.target_y,\n",
    "            logits = self.y,\n",
    "        ))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        self.train_step = optimizer.minimize(self.loss)\n",
    "\n",
    "        model_prediction = tf.argmax(input=self.y, axis=2)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(model_prediction, tf.cast(self.target_y,tf.int64)), tf.float32))\n",
    "\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        self.result_file = os.path.join(\"results\",\"har\",\"{}_{}.csv\".format(model_type,model_size))\n",
    "        if(not os.path.exists(\"results/har\")):\n",
    "            os.makedirs(\"results/har\")\n",
    "        if(not os.path.isfile(self.result_file)):\n",
    "            with open(self.result_file,\"w\") as f:\n",
    "                f.write(\"best epoch, train loss, train accuracy, valid loss, valid accuracy, test loss, test accuracy\\n\")\n",
    "\n",
    "        self.checkpoint_path = os.path.join(\"tf_sessions\",\"har\",\"{}\".format(model_type))\n",
    "        if(not os.path.exists(\"tf_sessions/har\")):\n",
    "            os.makedirs(\"tf_sessions/har\")\n",
    "            \n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def save(self):\n",
    "        self.saver.save(self.sess, self.checkpoint_path)\n",
    "\n",
    "    def restore(self):\n",
    "        self.saver.restore(self.sess, self.checkpoint_path)\n",
    "\n",
    "\n",
    "    def fit(self,gesture_data,epochs,verbose=True,log_period=50):\n",
    "\n",
    "        best_valid_accuracy = 0\n",
    "        best_valid_stats = (0,0,0,0,0,0,0)\n",
    "        self.save()\n",
    "        for e in range(epochs):\n",
    "            if(e%log_period == 0):\n",
    "                test_acc,test_loss = self.sess.run([self.accuracy,self.loss],{self.x:gesture_data.test_x,self.target_y: gesture_data.test_y})\n",
    "                valid_acc,valid_loss = self.sess.run([self.accuracy,self.loss],{self.x:gesture_data.valid_x,self.target_y: gesture_data.valid_y})\n",
    "                # Accuracy metric -> higher is better\n",
    "                if(valid_acc > best_valid_accuracy and e > 0):\n",
    "                    best_valid_accuracy = valid_acc\n",
    "                    best_valid_stats = (\n",
    "                        e,\n",
    "                        np.mean(losses),np.mean(accs)*100,\n",
    "                        valid_loss,valid_acc*100,\n",
    "                        test_loss,test_acc*100\n",
    "                    )\n",
    "                    self.save()\n",
    "\n",
    "            losses = []\n",
    "            accs = []\n",
    "            for batch_x,batch_y in gesture_data.iterate_train(batch_size=16):\n",
    "                acc,loss,_ = self.sess.run([self.accuracy,self.loss,self.train_step],{self.x:batch_x,self.target_y: batch_y})\n",
    "                if(not self.constrain_op is None):\n",
    "                    self.sess.run(self.constrain_op)\n",
    "\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "\n",
    "            if(verbose and e%log_period == 0):\n",
    "                print(\"Epochs {:03d}, train loss: {:0.2f}, train accuracy: {:0.2f}%, valid loss: {:0.2f}, valid accuracy: {:0.2f}%, test loss: {:0.2f}, test accuracy: {:0.2f}%\".format(\n",
    "                    e,\n",
    "                    np.mean(losses),np.mean(accs)*100,\n",
    "                    valid_loss,valid_acc*100,\n",
    "                    test_loss,test_acc*100\n",
    "                ))\n",
    "            if(e > 0 and (not np.isfinite(np.mean(losses)))):\n",
    "                break\n",
    "        self.restore()\n",
    "        best_epoch,train_loss,train_acc,valid_loss,valid_acc,test_loss,test_acc = best_valid_stats\n",
    "        print(\"Best epoch {:03d}, train loss: {:0.2f}, train accuracy: {:0.2f}%, valid loss: {:0.2f}, valid accuracy: {:0.2f}%, test loss: {:0.2f}, test accuracy: {:0.2f}%\".format(\n",
    "            best_epoch,\n",
    "            train_loss,train_acc,\n",
    "            valid_loss,valid_acc,\n",
    "            test_loss,test_acc\n",
    "        ))\n",
    "        with open(self.result_file,\"a\") as f:\n",
    "            f.write(\"{:03d}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}\\n\".format(\n",
    "            best_epoch,\n",
    "            train_loss,train_acc,\n",
    "            valid_loss,valid_acc,\n",
    "            test_loss,test_acc\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5217fac808372f2f6505ae6f54f4128aab8c8f29cf736dc8d94f727f2e56f7e7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
