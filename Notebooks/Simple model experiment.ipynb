{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from random import randint, seed\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-mounting",
   "metadata": {},
   "source": [
    "# Feature Extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_centroid(hand):\n",
    "    centroid = [0]*3\n",
    "    for i in range(3):\n",
    "        centroid[i] = hand[0][i] + hand[4][i] + hand[8][i] + hand[12][i] + hand[16][i] + hand[20][i]\n",
    "    centroid = list(x / 6 for x in centroid)\n",
    "    return centroid\n",
    "\n",
    "def get_active_fingers(hand):\n",
    "    centroid = get_centroid(hand)\n",
    "    \n",
    "    active = []\n",
    "    for i in range(1,6):\n",
    "        finger = hand[4*i]\n",
    "        active.append((finger[1] - centroid[1]) > 0)\n",
    "\n",
    "    return active\n",
    "\n",
    "def get_bent_fingers(hand):\n",
    "    centroid = get_centroid(hand)\n",
    "    bent = []    \n",
    "    \n",
    "    sign = -1 * math.floor(hand[4][0] - centroid[0])\n",
    "    bent.append((sign * (hand[4][0] - hand[3][0])) > 0)\n",
    "    for i in range(2,6):\n",
    "        bent.append(hand[4*i][1] < hand[4*i - 1][1])\n",
    "\n",
    "    return bent\n",
    "\n",
    "def bent_active(window):\n",
    "    instance = []\n",
    "    names = []\n",
    "    for frame_ind in range(len(window)):\n",
    "        frame = window[frame_ind]\n",
    "        \n",
    "        hand1, hand2 = frame[:63], frame[63:126]\n",
    "        hand1, hand2 = hand1.reshape(21, 3), hand2.reshape(21, 3)\n",
    "        hand1_active, hand1_bent = get_active_fingers(hand1), get_bent_fingers(hand1)\n",
    "        hand2_active, hand2_bent = get_active_fingers(hand2), get_bent_fingers(hand2)\n",
    "        \n",
    "        lists = [hand1_active, hand2_active, hand1_bent, hand2_bent]\n",
    "        for l in lists:\n",
    "            instance.extend(l)\n",
    "            \n",
    "        for string in [\"frame {} rhand_active {}\", \"frame {} lhand_active {}\", \"frame {} rhand_bent {}\", \"frame {} lhand_bent {}\"]:\n",
    "            for i in range(len(hand1_active)):\n",
    "                names.append(string.format(frame_ind, i))\n",
    "                \n",
    "    return instance, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes finite and small size, e.g. 5-10 frames\n",
    "def time_vectors(window):\n",
    "    vectors = []\n",
    "    lengths_3d = []\n",
    "    lengths_2d = []\n",
    "    angles = []\n",
    "    for frame_ind in range(len(window) - 1):\n",
    "        frame_1, frame_2 = window[frame_ind], window[frame_ind+1]\n",
    "        for point_ind in range(0, 42*3, 3):\n",
    "            a, b = frame_1[point_ind:point_ind+3], frame_2[point_ind:point_ind+3]\n",
    "            v = a - b\n",
    "            vectors.extend(v)\n",
    "            lengths_3d.append(np.linalg.norm(v))\n",
    "            \n",
    "            v = v[:-1]\n",
    "            lengths_2d.append(np.linalg.norm(v))\n",
    "            angles.append(np.arctan(v[1]/v[0] if v[0] != 0 else 0))\n",
    "    \n",
    "    vectors.extend(lengths_3d)\n",
    "    vectors.extend(lengths_2d)\n",
    "    vectors.extend(angles)\n",
    "    \n",
    "    names = []\n",
    "    for i in range(len(window) - 1):\n",
    "        for pt_ind in range(42):\n",
    "            names.append(f'frame {i}-{i+1} diff_x pt {pt_ind}')\n",
    "            names.append(f'frame {i}-{i+1} diff_y pt {pt_ind}')\n",
    "            names.append(f'frame {i}-{i+1} diff_z pt {pt_ind}')\n",
    "    \n",
    "    for string in [\"frame {}-{} pt {} len_3d\", \"frame {}-{} pt {} len_2d\", \"frame {}-{} pt {} angle\"]:\n",
    "        for i in range(len(window) - 1):\n",
    "            for pt_ind in range(42):\n",
    "                names.append(string.format(i, i+1, pt_ind))\n",
    "\n",
    "    return vectors, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fingertip_dists(window):\n",
    "    right_inds, left_inds = [12, 24, 36, 48, 60], [75, 87, 99, 111, 123]\n",
    "    tip_dists = []\n",
    "    names = []\n",
    "    \n",
    "    for frame_ind in range(len(window)):\n",
    "        frame = window[frame_ind]\n",
    "        \n",
    "        for i in range(len(right_inds)):\n",
    "            for j in range(i+1, len(right_inds)):\n",
    "                a_ind, b_ind = right_inds[i], right_inds[j]\n",
    "                a = frame[a_ind:a_ind+3]\n",
    "                b = frame[b_ind:b_ind+3]\n",
    "                tip_dists.append(np.linalg.norm(a-b))\n",
    "                names.append(f'frame {frame_ind} dist {a_ind}-{b_ind}')\n",
    "\n",
    "        for i in range(len(left_inds)):\n",
    "            for j in range(i+1, len(left_inds)):\n",
    "                a_ind, b_ind = left_inds[i], left_inds[j]\n",
    "                a = frame[a_ind:a_ind+3]\n",
    "                b = frame[b_ind:b_ind+3]\n",
    "                tip_dists.append(np.linalg.norm(a-b))\n",
    "                names.append(f'frame {frame_ind} dist {a_ind}-{b_ind}')\n",
    "    return tip_dists, names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-details",
   "metadata": {},
   "source": [
    "# Data cleaning and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = './collated/'\n",
    "min_samples = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(os.listdir(dir_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-cylinder",
   "metadata": {},
   "source": [
    "## Filter scenes with too many bad frames (more than 1/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_filter(window):\n",
    "    num = 0\n",
    "    for frame in window:\n",
    "        if (frame[0] == 0) and (frame[3] == 0) and (frame[6] == 0):\n",
    "            num += 1\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "rej_dict = {}\n",
    "for label in labels:\n",
    "    file_path = dir_name + label\n",
    "    scenes = pd.read_pickle(file_path)\n",
    "    print('-'*50)\n",
    "    print(\"Label: \" + label)\n",
    "    \n",
    "    rej_inds = []\n",
    "    for i, window in enumerate(scenes):\n",
    "        num_rej = window_filter(window)\n",
    "        if num_rej > 0:\n",
    "            print(\"REJ: {} bad frames out of \".format(num_rej), end='')\n",
    "        print(len(window), i)\n",
    "        if (num_rej/len(window)) > 0.2:\n",
    "            rej_inds.append(i)\n",
    "    print(\"REJ WINDOWS:\", rej_inds)\n",
    "    rej_dict[label] = rej_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = './filtered/'\n",
    "\n",
    "for label in labels:\n",
    "    file_path = dir_name + label\n",
    "    scenes = pd.read_pickle(file_path)\n",
    "    \n",
    "    valid_scenes = []\n",
    "    for i, window in enumerate(scenes):\n",
    "        num_rej = window_filter(window)\n",
    "        if (num_rej / len(window)) > 0.2:\n",
    "            print(\"REJ\")\n",
    "        else:\n",
    "            valid_scenes.append(window)\n",
    "    \n",
    "    print(f'Saving {len(valid_scenes)} scenes out of {len(scenes)} for {label}')\n",
    "    out_path = out_dir + label\n",
    "    pickle.dump(valid_scenes, open(out_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-lyric",
   "metadata": {},
   "source": [
    "## Filter bad frames from each scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_scene(window):\n",
    "    new_window = []\n",
    "    for frame in window:\n",
    "        if (frame[0] == 0) and (frame[3] == 0) and (frame[6] == 0):\n",
    "            pass\n",
    "        else:\n",
    "            new_window.append(frame)\n",
    "    return new_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-ending",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir_name = './filtered/'\n",
    "out_dir = './cleaned/'\n",
    "\n",
    "len_dict = {}\n",
    "for label in labels:\n",
    "    file_path = dir_name + label\n",
    "    scenes = pd.read_pickle(file_path)\n",
    "    \n",
    "    cleaned_scenes = [clean_scene(window) for window in scenes]\n",
    "    \n",
    "    print(f'Saving cleaned scenes of {label}')\n",
    "    len_dict[label] = len(cleaned_scenes)\n",
    "    out_path = out_dir + label\n",
    "    pickle.dump(cleaned_scenes, open(out_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-little",
   "metadata": {},
   "source": [
    "# Preprocessing and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1234)\n",
    "np_random = np.random.default_rng(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-tribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, last and x-2 uniform frames in between\n",
    "\n",
    "def uniform_sample(arr, x):\n",
    "    res = []\n",
    "    size = len(arr)\n",
    "    gap = size // (x - 1)\n",
    "    for i in range(x - 1):\n",
    "        res.append(arr[i * gap])\n",
    "    res.append(arr[-1])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 from x uniform segments\n",
    "\n",
    "def random_sample(arr, x):\n",
    "    res = []\n",
    "    size = len(arr)\n",
    "    gap = math.floor(size / x)\n",
    "    \n",
    "    for i in range(x):\n",
    "        r = min(randint(i * gap, (i+1) * gap), size - 1)\n",
    "        res.append(arr[r])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-copper",
   "metadata": {},
   "source": [
    "## Split into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_labels = ['look', 'same', 'cancel', 'devil', 'dress', 'live']\n",
    "label_map = {x: i for i, x in enumerate(selected_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scenes = []\n",
    "test_scenes = []\n",
    "\n",
    "file_format = \"./cleaned/{}_scenes.pkl\"\n",
    "for label in selected_labels:\n",
    "    file_path = file_format.format(label)\n",
    "    \n",
    "    scenes = pd.read_pickle(file_path)\n",
    "    \n",
    "    test_size = math.ceil(len(scenes) * 0.3)\n",
    "    test_inds = np_random.choice(len(scenes), test_size)\n",
    "    \n",
    "    for i, scene in enumerate(scenes):\n",
    "        if i in test_inds:\n",
    "            test_scenes.append((scene, label))\n",
    "        else:\n",
    "            train_scenes.append((scene, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-tragedy",
   "metadata": {},
   "source": [
    "## Up/down sample and Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_to_instance_w_names(window):\n",
    "    bents, names1 = bent_active(window)\n",
    "    times, names2 = time_vectors(window)\n",
    "    dists, names3 = fingertip_dists(window)\n",
    "    \n",
    "    bents.extend(times)\n",
    "    bents.extend(dists)\n",
    "    \n",
    "    names1.extend(names2)\n",
    "    names1.extend(names3)\n",
    "    \n",
    "    return bents, names1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_to_instance(window):\n",
    "    bents, _ = bent_active(window)\n",
    "    times, _ = time_vectors(window)\n",
    "    dists, _ = fingertip_dists(window)\n",
    "        \n",
    "    bents.extend(times)\n",
    "    bents.extend(dists)\n",
    "    \n",
    "    return bents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_choose = 5    # 5 seems fair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split, down sample and merge\n",
    "\n",
    "look_scenes = []\n",
    "other_scenes = []\n",
    "for tup in train_scenes:\n",
    "    if tup[1] == 'look':\n",
    "        look_scenes.append(tup)\n",
    "    else:\n",
    "        other_scenes.append(tup)\n",
    "        \n",
    "chosen_inds = np_random.choice(len(look_scenes), 20)    # 20 seems fair\n",
    "for i in chosen_inds:\n",
    "    other_scenes.append(look_scenes[i])\n",
    "\n",
    "X_down, y_down = [], []\n",
    "X_names = None\n",
    "for scene, label in other_scenes:\n",
    "    label_id = label_map[label]\n",
    "    \n",
    "    y_down.append(label_id)\n",
    "    \n",
    "    window = uniform_sample(scene, frames_choose)\n",
    "    if X_names == None:\n",
    "        instance, names = window_to_instance_w_names(window)\n",
    "        X_names = names\n",
    "    else:\n",
    "        instance = window_to_instance(window)\n",
    "    X_down.append(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample less freq classes to triple their instance\n",
    "\n",
    "X_up, y_up = [], []\n",
    "for scene, label in train_scenes:\n",
    "    label_id = label_map[label]\n",
    "    \n",
    "    windows = []\n",
    "    if label == 'look':\n",
    "        windows.append(uniform_sample(scene, frames_choose))\n",
    "    else:\n",
    "        for i in range(3):\n",
    "            windows.append(random_sample(scene, frames_choose))\n",
    "    \n",
    "    for window in windows:\n",
    "        instance = window_to_instance(window)\n",
    "        X_up.append(instance)\n",
    "        y_up.append(label_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-lloyd",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = [], []\n",
    "for scene, label in test_scenes:\n",
    "    label_id = label_map[label]\n",
    "    \n",
    "    y_test.append(label_id)\n",
    "    \n",
    "    window = uniform_sample(scene, frames_choose)\n",
    "    X_test.append(window_to_instance(window))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-degree",
   "metadata": {},
   "source": [
    "## Save to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_up = np.array(X_up)\n",
    "y_up = np.array(y_up)\n",
    "X_down = np.array(X_down)\n",
    "y_down = np.array(y_down)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_up\", X_up)\n",
    "np.save(\"y_up\", y_up)\n",
    "np.save(\"X_down\", X_down)\n",
    "np.save(\"y_down\", y_down)\n",
    "np.save(\"X_test\", X_test)\n",
    "np.save(\"y_test\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_names, open(\"X_names.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-moderator",
   "metadata": {},
   "source": [
    "# Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_up = np.load(\"X_up.npy\")\n",
    "y_up = np.load(\"y_up.npy\")\n",
    "X_down = np.load(\"X_down.npy\")\n",
    "y_down = np.load(\"y_down.npy\")\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_names = pickle.load(open(\"X_names.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_labels = ['look', 'same', 'cancel', 'devil', 'dress', 'live']\n",
    "label_map = {i: x for i, x in enumerate(selected_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_actual, y_predicted):\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_actual, y_predicted)\n",
    "\n",
    "    metrics_df = pd.DataFrame()\n",
    "\n",
    "    metrics_df[\"Class\"] = [label_map[x] for x in range(len(precision))]\n",
    "    metrics_df[\"Precision\"] = precision\n",
    "    metrics_df[\"Recall\"] = recall\n",
    "    metrics_df[\"F Score\"] = fscore\n",
    "\n",
    "    avg = [\"Average\",]\n",
    "    avg.extend(metrics_df.mean(axis=0, numeric_only=True))\n",
    "    metrics_df.loc[len(metrics_df)] = avg\n",
    "    \n",
    "    num_correct = 0\n",
    "    for i in range(len(y_actual)):\n",
    "        if y_actual[i] == y_predicted[i]:\n",
    "            num_correct += 1\n",
    "\n",
    "    return metrics_df, (num_correct / len(y_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_up = DecisionTreeClassifier()\n",
    "knn_up = KNeighborsClassifier()\n",
    "\n",
    "dtc_up.fit(X_up, y_up)\n",
    "knn_up.fit(X_up, y_up)\n",
    "\n",
    "dtc_up_pred = dtc_up.predict(X_test)\n",
    "knn_up_pred = knn_up.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_up_metrics, dtc_up_acc = get_metrics(y_test, dtc_up_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_up_metrics, knn_up_acc = get_metrics(y_test, knn_up_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_down = DecisionTreeClassifier()\n",
    "knn_down = KNeighborsClassifier()\n",
    "\n",
    "dtc_down.fit(X_down, y_down)\n",
    "knn_down.fit(X_down, y_down)\n",
    "\n",
    "dtc_down_pred = dtc_down.predict(X_test)\n",
    "knn_down_pred = knn_down.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-armstrong",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtc_down_metrics, dtc_down_acc = get_metrics(y_test, dtc_down_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-comfort",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "knn_down_metrics, knn_down_acc = get_metrics(y_test, knn_down_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:<6} {:<10} : {:0.8f}\".format(\"DTC\", \"Upsampled\", dtc_up_acc))\n",
    "print(\"{:<6} {:<10} : {:0.8f}\".format(\"DTC\", \"Downsampled\", dtc_down_acc))\n",
    "print(\"{:<6} {:<10} : {:0.8f}\".format(\"KNN\", \"Upsampled\", knn_up_acc))\n",
    "print(\"{:<6} {:<10} : {:0.8f}\".format(\"KNN\", \"Downsampled\", knn_down_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dtc_up_metrics)\n",
    "display(dtc_down_metrics)\n",
    "display(knn_up_metrics)\n",
    "display(knn_down_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, recall, fscore = dtc_up_metrics['Precision'], dtc_up_metrics['Recall'], dtc_up_metrics['F Score']\n",
    "\n",
    "xs = np.arange(len(prec)) + 0.5\n",
    "labels = [label_map[i] for i in range(len(prec) - 1)]\n",
    "labels.append(\"Average\")\n",
    "\n",
    "fig, axs = pyplot.subplots(2, 2, sharex=True, figsize=(15, 8))\n",
    "\n",
    "axs[0, 0].bar(xs, prec, tick_label=labels, color=\"tab:blue\")\n",
    "axs[0, 0].set_title(\"Precision\")\n",
    "axs[0, 0].set_ylim([0, 1.1])\n",
    "\n",
    "\n",
    "axs[0, 1].bar(xs, recall, tick_label=labels, color=\"tab:red\")\n",
    "axs[0, 1].set_title(\"Recall\")\n",
    "axs[0, 1].set_ylim([0, 1.1])\n",
    "\n",
    "axs[1, 0].bar(xs, fscore, tick_label=labels, color=\"tab:green\")\n",
    "axs[1, 0].set_title(\"F Score\")\n",
    "axs[1, 0].set_ylim([0, 1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, recall, fscore = knn_up_metrics['Precision'], knn_up_metrics['Recall'], knn_up_metrics['F Score']\n",
    "\n",
    "xs = np.arange(len(prec)) + 0.5\n",
    "labels = [label_map[i] for i in range(len(prec) - 1)]\n",
    "labels.append(\"Average\")\n",
    "\n",
    "fig, axs = pyplot.subplots(2, 2, sharex=True, figsize=(15, 8))\n",
    "\n",
    "axs[0, 0].bar(xs, prec, tick_label=labels, color=\"tab:blue\")\n",
    "axs[0, 0].set_title(\"Precision\")\n",
    "axs[0, 0].set_ylim([0, 1.1])\n",
    "\n",
    "\n",
    "axs[0, 1].bar(xs, recall, tick_label=labels, color=\"tab:red\")\n",
    "axs[0, 1].set_title(\"Recall\")\n",
    "axs[0, 1].set_ylim([0, 1.1])\n",
    "\n",
    "axs[1, 0].bar(xs, fscore, tick_label=labels, color=\"tab:green\")\n",
    "axs[1, 0].set_title(\"F Score\")\n",
    "axs[1, 0].set_ylim([0, 1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_feats = []\n",
    "for i, x in enumerate(dtc_up.feature_importances_):\n",
    "    if not x == 0:\n",
    "        important_feats.append((X_names[i], x))\n",
    "        \n",
    "important_feats.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"{:<25}: {:>10}\".format(\"Feature\", \"Importance\"))\n",
    "print(\"-\"*50)\n",
    "for name, val in important_feats:\n",
    "    print(\"{:<25}: {:0.8f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_feats = []\n",
    "for i, x in enumerate(dtc_down.feature_importances_):\n",
    "    if not x == 0:\n",
    "        important_feats.append((X_names[i], x))\n",
    "        \n",
    "important_feats.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"{:<25}: {:>10}\".format(\"Feature\", \"Importance\"))\n",
    "print(\"-\"*50)\n",
    "for name, val in important_feats:\n",
    "    print(\"{:<25}: {:0.8f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-fabric",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
